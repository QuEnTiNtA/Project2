{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1a0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from Network import *\n",
    "from DataSet import *\n",
    "from train_procedure import *\n",
    "from Aug import *\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abfe79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "satImage_100.png: 100%|████████████████████| 100/100 [00:56<00:00,  1.78image/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the augmented dataset\n",
    "create_augmented_dataset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a159af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a493e4",
   "metadata": {},
   "source": [
    "## Test on one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ef00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train ={\"save_model\": False,\n",
    "            \"cross_val\": True,\n",
    "            \"skip_connection\": True,\n",
    "            \"num_epochs\": 1,\n",
    "            \"n_splits\": 2,\n",
    "            \"batch_size\": 10,\n",
    "            \"dict_double_conv\": {\"BatchNorm\": True,\n",
    "                \"activation\": nn.ReLU(inplace=True),\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": False},\n",
    "            \"dict_ups\": {\"BatchNorm\": False,\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": False},\n",
    "            \"loss\": nn.BCEWithLogitsLoss(),\n",
    "            \"optimizer\": optim.Adam,\n",
    "            \"param_optimizer\": {\"weight_decay\": None,\n",
    "                               \"lr\": 1e-04},\n",
    "            \"use_scheduler\": True,\n",
    "            \"type_scheduler\": \"StepLR\",\n",
    "            \"scheduler\": torch.optim.lr_scheduler.StepLR,\n",
    "            \"param_scheduler\": {\"step_size\": 4,\n",
    "                               \"gamma\": 0.1},\n",
    "             \"scaler\": torch.cuda.amp.GradScaler(),\n",
    "             \"device\": DEVICE\n",
    "            }\n",
    "\n",
    "\n",
    "experiment = {\"param\": dict_train}\n",
    "experiment[\"convergence_path\"] = run_training(dict_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25576c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"result_exp/test.pkl\",\"wb\")\n",
    "pickle.dump(experiment,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c879cec9",
   "metadata": {},
   "source": [
    "## Experiment 1 Bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train1bis ={\"save_model\": False,\n",
    "            \"cross_val\": True,\n",
    "            \"skip_connection\": True,\n",
    "            \"num_epochs\": 10,\n",
    "            \"n_splits\": 2,\n",
    "            \"batch_size\": 10,\n",
    "            \"scale_channel\": 2,\n",
    "            \"dict_double_conv\": {\"BatchNorm\": True,\n",
    "                \"activation\": nn.ELU(inplace=True),\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": True},\n",
    "            \"dict_ups\": {\"BatchNorm\": False,\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": False},\n",
    "            \"loss\": nn.BCEWithLogitsLoss(),\n",
    "            \"optimizer\": optim.Adam,\n",
    "            \"param_optimizer\": {\"weight_decay\": None,\n",
    "                               \"lr\": 5e-04},\n",
    "            \"use_scheduler\": True,\n",
    "            \"type_scheduler\": \"StepLR\",\n",
    "            \"scheduler\": torch.optim.lr_scheduler.StepLR,\n",
    "            \"param_scheduler\": {\"step_size\": 4,\n",
    "                               \"gamma\": 0.1},\n",
    "             \"scaler\": torch.cuda.amp.GradScaler(),\n",
    "             \"device\": DEVICE\n",
    "            }\n",
    "\n",
    "\n",
    "experiment1bis = {\"param\": dict_train1bis}\n",
    "experiment1bis[\"convergence_path\"] = run_training(dict_train1bis) \n",
    "\n",
    "\n",
    "f = open(\"result_exp/experiment1bis.pkl\",\"wb\")\n",
    "pickle.dump(experiment1bis,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dcfb5e",
   "metadata": {},
   "source": [
    "## Experiment 2 Bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e41f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here scale_chanel = 1 for the network without skip connection since I forgot to decrease the number of \n",
    "# channel of this netwoork in the previous experiment\n",
    "\n",
    "dict_train2bis ={\"save_model\": False,\n",
    "            \"cross_val\": True,\n",
    "            \"skip_connection\": False,\n",
    "            \"num_epochs\": 10,\n",
    "            \"n_splits\": 2,\n",
    "            \"batch_size\": 10,\n",
    "            \"scale_channel\": 1,\n",
    "            \"dict_double_conv\": {\"BatchNorm\": True,\n",
    "                \"activation\": nn.ELU(inplace=True),\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": True},\n",
    "            \"dict_ups\": {\"BatchNorm\": False,\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": False},\n",
    "            \"loss\": nn.BCEWithLogitsLoss(),\n",
    "            \"optimizer\": optim.Adam,\n",
    "            \"param_optimizer\": {\"weight_decay\": None,\n",
    "                               \"lr\": 5e-04},\n",
    "            \"use_scheduler\": True,\n",
    "            \"type_scheduler\": \"StepLR\",\n",
    "            \"scheduler\": torch.optim.lr_scheduler.StepLR,\n",
    "            \"param_scheduler\": {\"step_size\": 4,\n",
    "                               \"gamma\": 0.1},\n",
    "             \"scaler\": torch.cuda.amp.GradScaler(),\n",
    "             \"device\": DEVICE\n",
    "            }\n",
    "\n",
    "\n",
    "experiment2bis = {\"param\": dict_train2bis}\n",
    "experiment2bis[\"convergence_path\"] = run_training(dict_train2bis) \n",
    "\n",
    "\n",
    "f = open(\"result_exp/experiment2bis.pkl\",\"wb\")\n",
    "pickle.dump(experiment2bis,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a3352",
   "metadata": {},
   "source": [
    "## Experiment 3 Bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b18049",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train3bis ={\"save_model\": False,\n",
    "            \"cross_val\": True,\n",
    "            \"skip_connection\": True,\n",
    "            \"num_epochs\": 10,\n",
    "            \"n_splits\": 2,\n",
    "            \"batch_size\": 10,\n",
    "            \"scale_channel\": 0.5,\n",
    "            \"dict_double_conv\": {\"BatchNorm\": True,\n",
    "                \"activation\": nn.ELU(inplace=True),\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": True},\n",
    "            \"dict_ups\": {\"BatchNorm\": False,\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": False},\n",
    "            \"loss\": nn.BCEWithLogitsLoss(),\n",
    "            \"optimizer\": optim.Adam,\n",
    "            \"param_optimizer\": {\"weight_decay\": None,\n",
    "                               \"lr\": 5e-04},\n",
    "            \"use_scheduler\": True,\n",
    "            \"type_scheduler\": \"StepLR\",\n",
    "            \"scheduler\": torch.optim.lr_scheduler.StepLR,\n",
    "            \"param_scheduler\": {\"step_size\": 4,\n",
    "                               \"gamma\": 0.1},\n",
    "             \"scaler\": torch.cuda.amp.GradScaler(),\n",
    "             \"device\": DEVICE\n",
    "            }\n",
    "\n",
    "\n",
    "experiment3bis = {\"param\": dict_train3bis}\n",
    "#experiment3bis[\"convergence_path\"] = run_training(dict_train3bis) \n",
    "\n",
    "\n",
    "f = open(\"result_exp/experiment3bis.pkl\",\"wb\")\n",
    "pickle.dump(experiment3bis,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9855b",
   "metadata": {},
   "source": [
    "## Experiment 4 Bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train4bis ={\"save_model\": False,\n",
    "            \"cross_val\": True,\n",
    "            \"skip_connection\": False,\n",
    "            \"num_epochs\": 10,\n",
    "            \"n_splits\": 2,\n",
    "            \"batch_size\": 10,\n",
    "            \"scale_channel\": 0.5,\n",
    "            \"dict_double_conv\": {\"BatchNorm\": True,\n",
    "                \"activation\": nn.ELU(inplace=True),\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": True},\n",
    "            \"dict_ups\": {\"BatchNorm\": False,\n",
    "                \"p_dropout\": 0.2,\n",
    "                \"use_dropout\": False,\n",
    "                \"bias\": False},\n",
    "            \"loss\": nn.BCEWithLogitsLoss(),\n",
    "            \"optimizer\": optim.Adam,\n",
    "            \"param_optimizer\": {\"weight_decay\": None,\n",
    "                               \"lr\": 5e-04},\n",
    "            \"use_scheduler\": True,\n",
    "            \"type_scheduler\": \"StepLR\",\n",
    "            \"scheduler\": torch.optim.lr_scheduler.StepLR,\n",
    "            \"param_scheduler\": {\"step_size\": 4,\n",
    "                               \"gamma\": 0.1},\n",
    "             \"scaler\": torch.cuda.amp.GradScaler(),\n",
    "             \"device\": DEVICE\n",
    "            }\n",
    "\n",
    "\n",
    "experiment4bis = {\"param\": dict_train4bis}\n",
    "experiment4bis[\"convergence_path\"] = run_training(dict_train4bis) \n",
    "\n",
    "\n",
    "f = open(\"result_exp/experiment4bis.pkl\",\"wb\")\n",
    "pickle.dump(experiment4bis,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605bf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279a2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
